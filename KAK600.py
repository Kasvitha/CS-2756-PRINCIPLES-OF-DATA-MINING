# -*- coding: utf-8 -*-
"""AQI Data Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zESLtkDi2SmZpqT98UT7BAIanQTepJzO

#1. Data Preparation

##1.1 Import Libraries
"""

import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Deep Learning
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# XGBoost
import xgboost as xgb

# For ST-GNN
# import torch
# import torch.nn as nn
# from torch_geometric.nn import GCNConv


# Linear Regression
from sklearn.linear_model import LinearRegression

# Time Series Analysis
from statsmodels.tsa.arima.model import ARIMA

# Random Forest and Gradient Boosting
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

"""##1.2 Load the Dataset"""

# Load the dataset
# url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip'
data = pd.read_csv('AirQualityUCI.csv', sep=';', decimal=',', parse_dates=[['Date', 'Time']])

# Display the first few rows
data.head()

data.shape

"""##1.3 Data Cleaning"""

# Replace invalid values and drop unnecessary columns
data.replace(-200, np.nan, inplace=True)
data.dropna(axis=1, how='all', inplace=True)
data.dropna(axis=0, how='any', inplace=True)

# Specify the date format
date_format = '%d/%m/%Y %H.%M.%S'

# Convert 'Date_Time' to datetime using the specified format
data['Date_Time'] = pd.to_datetime(data['Date_Time'], format=date_format, errors='coerce')

# Drop rows where 'Date_Time' conversion failed
data.dropna(subset=['Date_Time'], inplace=True)

# Set 'Date_Time' as the index
data.set_index('Date_Time', inplace=True)

data.head()

"""##1.4 Feature Selection"""

# Select relevant features
features = ['CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)',
            'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)',
            'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)',
            'T', 'RH', 'AH']

# Target variable (Assuming 'NO2(GT)' as a proxy for AQI)
target = 'NO2(GT)'

# Create feature matrix X and target vector y
X = data[features].drop(columns=[target])
y = data[target]

"""##1.5 Handling Missing Values"""

# Impute missing values with mean
X.fillna(X.mean(), inplace=True)
y.fillna(y.mean(), inplace=True)

"""#Existing Models

#2. Linear Regression

##2.1 Split the Data
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

"""##2.2 Train the Model

"""

# Initialize the Linear Regression model
lr_model = LinearRegression()

# Train the model
lr_model.fit(X_train, y_train)

"""#2.3: Evaluate the Model"""

# Make predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluate the model
mse_lr = mean_squared_error(y_test, y_pred_lr)
print(f'Linear Regression MSE: {mse_lr}')

"""# 3. ARIMA Model

##3.1: Prepare Data for ARIMA
"""

# Ensure the target variable is a time series
ts_data = y

# Plot the time series
plt.figure(figsize=(12,5))
plt.plot(ts_data)
plt.title('NO2_GT Time Series')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT')
plt.show()

"""##3.2: Check for Stationarity"""

from statsmodels.tsa.stattools import adfuller

def adf_test(series):
    result = adfuller(series)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    for key, value in result[4].items():
        print('Critical Value (%s): %.3f' % (key, value))

# Perform ADF test
print('Results of Dickey-Fuller Test:')
adf_test(ts_data)

"""If the series is not stationary (p-value > 0.05), we need to difference it.

##3.3: Differencing to Achieve Stationarity
"""

# First-order differencing
ts_data_diff = ts_data.diff().dropna()

# Re-test for stationarity
print('Results of Dickey-Fuller Test after differencing:')
adf_test(ts_data_diff)

"""##3.4: Split the Differenced Data"""

# Split the differenced data
split_ratio = 0.8
split_index = int(len(X) * split_ratio)

train_data_arima = ts_data_diff.iloc[:split_index]
test_data_arima = ts_data_diff.iloc[split_index:]

"""#3.5: Fit the ARIMA Model"""

# Import ACF and PACF plots
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Plot ACF and PACF
fig, axes = plt.subplots(1,2, figsize=(16,4))
plot_acf(train_data_arima, ax=axes[0])
plot_pacf(train_data_arima, ax=axes[1])
plt.show()

# Based on plots, we can choose ARIMA order (p,d,q)
# For simplicity, let's use (1,1,1)

"""##3.6: Train the Model"""

!pip install pmdarima

from pmdarima import auto_arima

# Automatically determine the best (p, d, q) values
model = auto_arima(ts_data[:split_index], seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)
print(model.summary())

# Fit with the suggested parameters
arima_order = model.order
arima_model = ARIMA(ts_data[:split_index], order=arima_order)
arima_model_fit = arima_model.fit()
print(arima_model_fit.summary())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
ts_data_scaled = scaler.fit_transform(ts_data.values.reshape(-1, 1)).flatten()

# Fit ARIMA on scaled data
arima_model = ARIMA(ts_data_scaled[:split_index], order=(1, 1, 1))
arima_model_fit = arima_model.fit()

# Transform predictions back to the original scale after forecasting
arima_forecast_scaled = arima_model_fit.forecast(steps=len(ts_data) - split_index)
arima_forecast = scaler.inverse_transform(arima_forecast_scaled.reshape(-1, 1)).flatten()

arima_model = ARIMA(ts_data[:split_index], order=(1, 1, 1))
arima_model_fit = arima_model.fit()



"""##3.7: Make Predictions"""

# Forecast
forecast_steps = len(ts_data) - split_index
arima_forecast = arima_model_fit.forecast(steps=forecast_steps)

# Evaluate the model
actual_values = ts_data.iloc[split_index:]
mse_arima = mean_squared_error(actual_values, arima_forecast)
print(f'ARIMA Model MSE: {mse_arima}')

data['NO2(GT)']

target = 'NO2(GT)'

# Scaling the data
scaler = StandardScaler()
ts_data_scaled = scaler.fit_transform(data[[target]].values).flatten()

# Split the data into training and testing sets
split_ratio = 0.8
split_index = int(len(ts_data_scaled) * split_ratio)
train_data = ts_data_scaled[:split_index]
test_data = ts_data_scaled[split_index:]

# Step 1: Determine optimal (p, d, q) order using auto_arima
auto_model = auto_arima(train_data, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)
arima_order = auto_model.order
print(f"Optimal ARIMA order: {arima_order}")

# Step 2: Fit the ARIMA model with the optimal order
arima_model = ARIMA(train_data, order=arima_order)
arima_model_fit = arima_model.fit()
print(arima_model_fit.summary())

# Step 3: Forecast for the test set length
forecast_steps = len(test_data)
arima_forecast_scaled = arima_model_fit.forecast(steps=forecast_steps)

# Step 4: Inverse transform the forecast to get back to the original scale
arima_forecast = scaler.inverse_transform(arima_forecast_scaled.reshape(-1, 1)).flatten()
actual_values = scaler.inverse_transform(test_data.reshape(-1, 1)).flatten()

# Calculate Mean Squared Error
mse_arima = mean_squared_error(actual_values, arima_forecast)
print(f"ARIMA Model MSE: {mse_arima}")

# Plot the actual vs predicted values
plt.figure(figsize=(15, 7))
plt.plot(data.index[split_index:], actual_values, label='Actual NO2_GT', color='black')
plt.plot(data.index[split_index:], arima_forecast, label='ARIMA Predictions', color='orange')
plt.title('Actual vs Predicted NO2_GT Levels (ARIMA)')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT Value')
plt.legend()
plt.show()

"""#4. Random Forest Regressor

##4.1 Train the Model
"""

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

"""##4.2 Evaluate the Model"""

y_pred_rf = rf_model.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Random Forest MSE: {mse_rf}')

"""#5. Gradient Boosting Regressor

##5.1: Train the Model
"""

# Initialize the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train the model
gb_model.fit(X_train, y_train)

"""##5.2: Evaluate the Model"""

# Make predictions
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
mse_gb = mean_squared_error(y_test, y_pred_gb)
print(f'Gradient Boosting MSE: {mse_gb}')

"""#6. Existing Models Results Comparison"""

print('Model Performance Metrics (MSE):')
print(f'Linear Regression: {mse_lr}')
print(f'ARIMA Model: {mse_arima}')
print(f'Random Forest: {mse_rf}')
print(f'Gradient Boosting: {mse_gb}')

"""##6.1 Visualization of Predictions"""

len(y_test.index)

plt.figure(figsize=(15,7))

# Plot actual values
plt.plot(y_test.index, y_test.values, label='Actual NO2_GT', color='black')

# Plot predictions from each model
plt.plot(y_test.index, y_pred_lr, label='Linear Regression', alpha=0.7)
plt.plot(data.index[split_index:], arima_forecast, label='ARIMA Predictions', color='orange')
plt.plot(y_test.index, y_pred_rf, label='Random Forest', alpha=0.7)
plt.plot(y_test.index, y_pred_gb, label='Gradient Boosting', alpha=0.7)

plt.title('Actual vs Predicted NO2_GT Levels')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT Value')
plt.legend()
plt.show()

"""#3. Long Short-Term Memory (LSTM) Networks

##3.1 Prepare Data for LSTM
"""

from sklearn.preprocessing import MinMaxScaler

# Scale the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Reshape data for LSTM [samples, timesteps, features]
timesteps = 1
X_lstm = X_scaled.reshape((X_scaled.shape[0], timesteps, X_scaled.shape[1]))

# Split the data
X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y, test_size=0.2, shuffle=False)

X_train_lstm.shape

"""##3.2 Build the LSTM Model"""

model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(timesteps, X_lstm.shape[2])))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

"""##3.3 Train the Model"""

history = model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test_lstm))

"""##3.4 Evaluate the Model"""

y_pred_lstm = model.predict(X_test_lstm)
mse_lstm = mean_squared_error(y_test_lstm, y_pred_lstm)
print(f'LSTM MSE: {mse_lstm}')

"""#4. XGBoost Regressor

##4.1 Train the Model
"""

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)
xgb_model.fit(X_train, y_train)

"""##4.2 Evaluate the Model"""

y_pred_xgb = xgb_model.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost MSE: {mse_xgb}')

print('Model Performance Metrics (MSE):')
print(f'Linear Regression: {mse_lr}')
print(f'ARIMA Model: {mse_arima}')
print(f'Random Forest: {mse_rf}')
print(f'Gradient Boosting: {mse_gb}')
print(f'LSTM: {mse_lstm}')
print(f'XGBoost: {mse_xgb}')

plt.figure(figsize=(15,7))

# Plot actual values
plt.plot(y_test.index, y_test.values, label='Actual NO2_GT', color='black')

# Plot predictions from each model
plt.plot(y_test.index, y_pred_lr, label='Linear Regression', alpha=0.7)
plt.plot(data.index[split_index:], arima_forecast, label='ARIMA Predictions', color='orange')
plt.plot(y_test.index, y_pred_rf, label='Random Forest', alpha=0.7)
plt.plot(y_test.index, y_pred_gb, label='Gradient Boosting', alpha=0.7)
plt.plot(y_test.index, y_pred_lstm, label='LSTM', alpha=0.7)
plt.plot(y_test.index, y_pred_xgb, label='XGBoost', alpha=0.7)

plt.title('Actual vs Predicted NO2_GT Levels')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT Value')
plt.legend()
plt.show()







model = Sequential()
model.add(LSTM(50, activation='tanh', input_shape=(timesteps, X_lstm.shape[2])))
model.add(Dense(1))  # Single output for regression tasks
model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(
    X_train_lstm,
    y_train_lstm,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Evaluate the model
loss = model.evaluate(X_test_lstm, y_test_lstm, verbose=1)
print(f"Test Loss: {loss}")

# Make predictions
predictions = model.predict(X_test_lstm)

plt.figure(figsize=(15,7))

# Plot actual values
plt.plot(y_test.index, y_test.values, label='Actual NO2_GT', color='black')

# Plot predictions from each model
plt.plot(y_test.index, y_pred_lr, label='Linear Regression', alpha=0.7)
plt.plot(data.index[split_index:], arima_forecast, label='ARIMA Predictions', color='orange')
plt.plot(y_test.index, y_pred_rf, label='Random Forest', alpha=0.7)
plt.plot(y_test.index, y_pred_gb, label='Gradient Boosting', alpha=0.7)
plt.plot(y_test.index, y_pred_lstm, label='LSTM1', alpha=0.7)
plt.plot(y_test.index, predictions, label='LSTM2', alpha=0.7)
plt.plot(y_test.index, y_pred_xgb, label='XGBoost', alpha=0.7)

plt.title('Actual vs Predicted NO2_GT Levels')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT Value')
plt.legend()
plt.show()

# Build a more complex LSTM model
model = Sequential()

# First LSTM layer with return_sequences=True
model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(timesteps, X_lstm.shape[2])))
model.add(Dropout(0.2))  # Dropout to prevent overfitting

# Second LSTM layer
model.add(LSTM(64, activation='tanh', return_sequences=True))
model.add(Dropout(0.2))

# Third LSTM layer without return_sequences
model.add(LSTM(32, activation='tanh'))
model.add(Dropout(0.2))

# Fully connected Dense layers
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Single output for regression tasks

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(
    X_train_lstm,
    y_train_lstm,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Evaluate the model
loss = model.evaluate(X_test_lstm, y_test_lstm, verbose=1)
print(f"Test Loss: {loss}")

# Make predictions
predictions2 = model.predict(X_test_lstm)

plt.figure(figsize=(15,7))

# Plot actual values
plt.plot(y_test.index, y_test.values, label='Actual NO2_GT', color='black')

# Plot predictions from each model
plt.plot(y_test.index, y_pred_lr, label='Linear Regression', alpha=0.7)
plt.plot(data.index[split_index:], arima_forecast, label='ARIMA Predictions', color='orange')
plt.plot(y_test.index, y_pred_rf, label='Random Forest', alpha=0.7)
plt.plot(y_test.index, y_pred_gb, label='Gradient Boosting', alpha=0.7)
plt.plot(y_test.index, y_pred_lstm, label='LSTM_Naive', alpha=0.7)
plt.plot(y_test.index, predictions, label='LSTM_Moderate', alpha=0.7)
plt.plot(y_test.index, predictions2, label='LSTM_Complex', alpha=0.7)
plt.plot(y_test.index, y_pred_xgb, label='XGBoost', alpha=0.7)

plt.title('Actual vs Predicted NO2_GT Levels')
plt.xlabel('Date_Time')
plt.ylabel('NO2_GT Value')
plt.legend()
plt.show()

print('Model Performance Metrics (MSE):')
print(f'Linear Regression: {mse_lr}')
print(f'ARIMA Model: {mse_arima}')
print(f'Random Forest: {mse_rf}')
print(f'Gradient Boosting: {mse_gb}')
print(f'LSTM_Naive: {mse_lstm}')
print(f'LSTM_Moderate: 3394.5303')
print(f'LSTM_Complex: 121.8156')
print(f'XGBoost: {mse_xgb}')

a